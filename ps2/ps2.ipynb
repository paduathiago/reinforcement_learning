{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aae8107-13ed-445a-be30-7ef6792f3fd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f61ccfb98ac7260495125199109df787",
     "grade": false,
     "grade_id": "cell-68384257accf953c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lista de Exercícios 2: Métodos baseados em Diferenças Temporais\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb4d62-c595-475e-bfd0-01668c703127",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d6a7fb94961f19e96d266e0a8baec78",
     "grade": false,
     "grade_id": "cell-4242dcc6c6ea89a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a8fd0-2b46-4f21-9107-37855e3600cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fd97c49944b331ec877108bbc23fb9b",
     "grade": false,
     "grade_id": "cell-65fa7679cf1f2fc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido **antes** de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook (*ps2.ipynb*) com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 29/04/2025. **Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.**\n",
    "- ***SUBMISSÕES QUE NÃO SEGUIREM ESTAS INSTRUÇÕES NÃO SERÃO AVALIADAS.***\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2099c-d4b6-4e74-a73e-e363f31da8c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d67145cf8c0e6a62279f3ed6b22603fd",
     "grade": false,
     "grade_id": "cell-01460896f26b4db3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Cliff Walking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27971bbb-2c94-4b00-9553-a1eed2c97b15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff87891829b78aebfee5f175506d5edb",
     "grade": false,
     "grade_id": "cell-0b1d654fac64a5c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Cliff Walking é um ambiente representado por um grid de tamanho $4 \\times 12$, no qual um agente precisa atravessar o mapa do canto inferior esquerdo até o canto inferior direito, evitando um perigoso penhasco na parte inferior do mapa. O mapa do Cliff Walking pode ser visto no gif abaixo.\n",
    "\n",
    "![Cliff Walking](https://gymnasium.farama.org/_images/cliff_walking.gif)\n",
    "\n",
    "O agente sempre inicia na posição $(3, 0)$ e seu objetivo é alcançar a posição $(3, 11)$. As células das três primeiras linhas do grid (linhas $0$, $1$ e $2$) são seguras e o agente pode se mover livremente por elas. Já a linha $3$ contém um penhasco: todas as posições de $(3, 1)$ a $(3, 10)$ representam zonas de risco. Se o agente entrar em uma dessas células, ele cai do penhasco, o que encerra imediatamente o episódio com uma penalidade significativa. A cada passo, o agente recebe uma observação indicando sua posição atual (representada por um valor inteiro) e tem a possibilidade de escolher entre quatro ações possíveis: mover-se para cima, para baixo, para a esquerda ou para a direita. Cada movimento acarreta uma penalidade de $-1$, com exceção das quedas no penhasco, que resultam em uma penalidade severa de $-100$. Um episódio termina quando o agente alcança o objetivo final ou cai do penhasco. Para mais detalhes sobre o ambiente leia a [documentação do gymnasium](https://gymnasium.farama.org/environments/toy_text/cliff_walking/).\n",
    "\n",
    "Nesta lista de exercícios, você irá trabalhar com o ambiente Cliff Walking descrito acima. Seu objetivo será implementar e analisar dois algoritmos baseados em diferenças temporais: Sarsa e Q-Learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f0dff-63ba-4b7e-bd8d-63e656cca8f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6b4351ea0438065598365d1ab8bd92b",
     "grade": false,
     "grade_id": "cell-5d63b959517bb0c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4dc6c0-4f5b-4865-870f-25a11475482a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "277126944288b1f588c82b9c061d9486",
     "grade": false,
     "grade_id": "cell-68f5d67a164f4a3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sua primeira tarefa consiste em implementar um agente baseado no algoritmo Sarsa, que deverá seguir uma política $\\varepsilon$-greedy. Para isso, utilize como referência o livro-texto da disciplina e os materiais discutidos em sala.\n",
    "\n",
    "Você deverá concluir a implementação da classe `SarsaAgent` conforme as instruções abaixo:\n",
    "\n",
    "1. Implemente o método `__init__` que inicializa um novo agente Sarsa. Ele deve receber como parâmetros o espaço de observações, o espaço de ações, a taxa de aprendizado $\\alpha$, o fator de desconto $\\gamma$, e o parâmetro de exploração $\\varepsilon$.\n",
    "2. Implemente o método `choose_action`, responsável por escolher uma ação a partir de um estado observado, seguindo a política $\\varepsilon$-greedy.\n",
    "3. Implemente o método `learn`, que atualiza os *Q-values* do agente com base na experiência obtida durante a interação com o ambiente.\n",
    "4. Implemente o método `train`, que executa o loop de treinamento do algoritmo Sarsa. O ambiente de treinamento e o número de episódios devem ser fornecidos como parâmetros de entrada. O método deve retornar dois elementos: (1) uma tabela contendo os *Q-values* calculados durante o treinamento; e (2) uma lista com a soma das recompensas obtidas ao longo de cada episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235bfa8-fe48-4839-8ab2-b79d3023c804",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae0858805a835d24ace81ab43392125c",
     "grade": false,
     "grade_id": "cell-4dd0e4f349b22072",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e6e44-8477-43d8-a0ab-64f8043b7147",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "314846f574df1e217f9244329d20fa7d",
     "grade": false,
     "grade_id": "cell-d636c499396d6ec7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, observation_space, action_space, alpha, gamma, epsilon):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d731e-0de5-49b2-b6f4-7dd2f09c7b18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e233b8139955e198a2defe3740572ec",
     "grade": false,
     "grade_id": "cell-709f13c64d6475a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "4. Agora, treine um novo agente Sarsa no ambiente Cliff Walking por 1000 episódios utilizando os seguintes parâmetros: taxa de aprendizado $\\alpha = 0.1$, fator de desconto $\\gamma = 0.9$ e parâmetro de exploração $\\varepsilon = 0.1$. Armazene a saída do método `train` nas variáveis `sarsa_q_table` e `sarsa_returns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd34919-e398-451c-ac55-bff3cfb1b2af",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba085207684bc3374f0989f862cd3f18",
     "grade": false,
     "grade_id": "cell-7e0c686c00dc5983",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361edd7-87ed-42ce-9b10-d1318a20e00c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11df9b7b44a26b5f25926abcf80cc4ae",
     "grade": true,
     "grade_id": "cell-6afadabd1049b26f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f5161-9fea-4abe-9176-4ef95fac44fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04e52015155946480f0c3ea06cb2586b",
     "grade": false,
     "grade_id": "cell-1399e7b3b3e29c23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nas células a seguir, analise com atenção a política gulosa obtida a partir do treinamento com o algoritmo Sarsa. Na última parte desta lista, você deverá responder algumas perguntas relacionadas a essa política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caeb6ce-abb6-4769-9594-c3dee4ce13d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b749baeada56ee55f9d36f1dbd1233a2",
     "grade": false,
     "grade_id": "cell-e24e9dff8cd68cf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_greedy_policy(q_table):\n",
    "\n",
    "    action_map = ['↑', '→', '↓', '←']\n",
    "    q_table = np.array(q_table)\n",
    "\n",
    "    if q_table.shape != (48, 4):\n",
    "        raise ValueError(\"Q-table must have shape (48, 4)\")\n",
    "\n",
    "    for row in range(4):\n",
    "        line = []\n",
    "        for col in range(12):\n",
    "            state = row * 12 + col\n",
    "            if row == 3:\n",
    "                char = (\n",
    "                    action_map[np.argmax(q_table[state])] if col == 0 else\n",
    "                    '◎' if col == 11 else\n",
    "                    '▢'\n",
    "                )\n",
    "            else:\n",
    "                char = action_map[np.argmax(q_table[state])]\n",
    "            line.append(char)\n",
    "        print(' '.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80d7cb-5e86-4adf-a4b0-786521d68db3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c55642c9b8d31b3903e2f114e0828be",
     "grade": false,
     "grade_id": "cell-e1a6acc118656a85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print_greedy_policy(sarsa_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16ce94-c44b-4cee-acd2-f39f79ef13ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4838ff3b7794e97fccfcadc5096d9c2b",
     "grade": false,
     "grade_id": "cell-906f7ada9ad2b69a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13413d4c-e1f6-40b2-85a6-731978832ac0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c66eeeebf84fee80fc9daf6871b1095",
     "grade": false,
     "grade_id": "cell-4c60cf29da13e58f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff526f-a824-4e88-91f3-980157c9b39e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8da119e9ff8ec19db16aad6debf36a9f",
     "grade": false,
     "grade_id": "cell-bc33218e2b7d02e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nesta atividade, você deverá implementar um agente baseado no algoritmo Q-learning, que também utiliza uma política $\\varepsilon$-greedy para explorar o ambiente durante o treinamento. Novamente, utilize o livro-texto da disciplina e os materiais discutidos em sala de aula como referência.\n",
    "\n",
    "Você deverá concluir a implementação da classe QLearningAgent, conforme as instruções a seguir:\n",
    "\n",
    "5. Implemente o método `__init__` que inicializa um novo agente Q-Learning. Ele deve receber como parâmetros o espaço de observações, o espaço de ações, a taxa de aprendizado $\\alpha$, o fator de desconto $\\gamma$, e o parâmetro de exploração $\\varepsilon$.\n",
    "6. Implemente a função `choose_action`, responsável por escolher uma ação a partir de um estado observado, seguindo a política $\\varepsilon$-greedy.\n",
    "7. Implemente a função `learn`, que atualiza os *Q-values* do agente com base na experiência obtida durante a interação com o ambiente.\n",
    "8. Implemente o método `train`, que executa o loop de treinamento do algoritmo Q-Learning. O ambiente de treinamento e o número de episódios devem ser fornecidos como parâmetros de entrada. O método deve retornar dois elementos: (1) uma tabela contendo os *Q-values* calculados durante o treinamento; e (2) uma lista com a soma das recompensas obtidas ao longo de cada episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95f1ca-0423-4a8f-8f18-4f80c8a064f7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b49bf1339ff760e94d63dd0eb1d99ec",
     "grade": false,
     "grade_id": "cell-944a5a77fb631900",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, observation_space, action_space, alpha, gamma, epsilon):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, env, episodes):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef68b30-2981-4a9c-9440-9d33e1570289",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b07efcc51b8fcefd7d1321711abce308",
     "grade": false,
     "grade_id": "cell-abc8e9bd9fd60320",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "9. Treine este novo agente por 1000 episódios no ambiente Cliff Walking. Utilize os mesmos parâmetros do exercício 4. Armazene a saída do método `train` nas variáveis `ql_q_table` e `ql_returns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41069fb-0ffd-47ee-9b48-a7f4a240ddf2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70828cf9176675d578b17b1d47054a3a",
     "grade": false,
     "grade_id": "cell-5679ac1bc6f2dfb6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bf8fa-2f6f-49d8-8af7-d122c657d2b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96f1da5f692a7b6bf72b5cb195885167",
     "grade": true,
     "grade_id": "cell-227e87e79d184378",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9b69c-8b33-4e51-b932-f5a77533c738",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66df7c354aac2c0ce598434fbd940e19",
     "grade": false,
     "grade_id": "cell-cffbafda5ce5dcfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analise com atenção a política gulosa obtida a partir do treinamento com o algoritmo Q-Learning. Na próxima parte desta lista, você deverá responder algumas perguntas relacionadas a essa política.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c394883-0eb7-4afa-9054-c7c91a59592c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e47c3926575c18a81caa1d82eadc8a06",
     "grade": false,
     "grade_id": "cell-de44aba61abdf811",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print_greedy_policy(ql_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c075e-80d7-447d-be90-9ff765ed464d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1073cf43a22e6d6218625092159be068",
     "grade": false,
     "grade_id": "cell-7a59336d133618bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947af98-a23c-4221-8797-a160d9d0767a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "112208feac5fc3b96d0d536794d63847",
     "grade": false,
     "grade_id": "cell-82458a36e1c5ac52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2e0a3-a543-4535-a84d-a49a7879dd22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f35ebd8ab5029f363d0e25ec4866fac",
     "grade": false,
     "grade_id": "cell-81a77e4be51f3b70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "10. As políticas obtidas pelos agentes Sarsa e Q-learning apresentam diferenças significativas? Explique por que essas diferenças ocorrem - ou por que não ocorrem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aa867-85a7-4145-ad4c-d0984938f45e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceece6b773a6929fa75ac6ad4b291ded",
     "grade": true,
     "grade_id": "cell-ed8c2f37fa98e0b9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198fe5a-8345-433c-b3bd-5fc471d95c29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43f92ba89275a603c71da8a6cb14bc72",
     "grade": false,
     "grade_id": "cell-61901e141ef88cb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Utilize a biblioteca *matplotlib* para construir um gráfico comparativo dos retornos episódicos obtidos pelos algoritmos Sarsa e Q-Learning. Utilize as variáveis `sarsa_returns` e `ql_returns` obtidas nos exercícios anteriores. No eixo X, represente os episódios; no eixo Y, o retorno acumulado por episódio. Lembre-se de incluir título e legendas apropriadas para facilitar a interpretação dos dados.\n",
    "\n",
    "**Importante:** O gráfico gerado deve se assemelhar ao exemplo abaixo. Embora os resultados dificilmente sejam idênticos, é fundamental que as tendências de cada algoritmo estejam bem evidentes. Caso os dados estejam muito ruidosos e dificultem a visualização das tendências, experimente aplicar uma média móvel ou um filtro gaussiano para suavizar as curvas.\n",
    "\n",
    "![Sarsa vs Q-Learning](sarsa-vs-qlearn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486b44c-9c2f-4cb2-95d8-e453afd2cb66",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37bc373a69e28c1e127fa96eeac7f804",
     "grade": true,
     "grade_id": "cell-a61177747fbf22fb",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdaf0d-44ee-48e0-8a71-ad2c1658948c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13151bf773a00ca4389aa86b9643895b",
     "grade": false,
     "grade_id": "cell-7025c9fa89d21506",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Realize um novo treinamento do agente SARSA, desta vez utilizando $\\varepsilon = 0$, ou seja, adotando uma política puramente gulosa. Preserve todos os outros parâmetros utilizados anteriormente e armazene a saída do método `train` nas variáveis `sarsa_q_table` e `sarsa_returns`. Ao final do treinamento, observe a política aprendida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996d8f1-559b-43b4-ba65-d2d5f2669369",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87c4c4aa8ba16d402a4179fba412a7ec",
     "grade": false,
     "grade_id": "cell-64f4962f9ab2a5c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "env.close()\n",
    "\n",
    "print_greedy_policy(sarsa_q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd0af9-62a7-4419-99c3-0d60f4a78f73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bb41e40844140334b3e7981275b0081",
     "grade": true,
     "grade_id": "cell-22f6eaf49cc06201",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81372206-8d1c-44f6-9d66-0875a8ce9d71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4920edba67f1c14206654d065652a955",
     "grade": false,
     "grade_id": "cell-9989ee28e6b79f49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Neste caso onde o valor do parâmetro de exploração $\\varepsilon$ é igual a zero, o algoritmo Sarsa se torna equivalente ao algoritmo Q-learning? Justifique sua resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c8870-1cad-4ad1-8265-e84a90b86440",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e35f6d3952749aca15c2954ac5578f2",
     "grade": true,
     "grade_id": "cell-ba93965f29322c00",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
