{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c18e0f-36ba-4010-8ba6-9ec2c9966e3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eead5b36a3ee2d8eb504615ec2e698b",
     "grade": false,
     "grade_id": "cell-4c87f565d23c35db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lista de Exercícios 1: Processos de Decisão de Markov e Programação Dinâmica\n",
    "\n",
    "#### Disciplina: Aprendizado por Reforço\n",
    "#### Professor: Luiz Chaimowicz\n",
    "#### Monitores: Marcelo Lemos e Ronaldo Vieira\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140e15d-8ff2-404e-9f0b-de4c9444c9a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46221e6cdc09eef7a6cc1ae625ff7263",
     "grade": false,
     "grade_id": "cell-e6ec4bda5dd7e12a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd360e8-7428-4cb3-af57-ef79d5c99ef8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19bd83f45fc7894e6b1b0838321195ad",
     "grade": false,
     "grade_id": "cell-d31fd315f36785a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Leia atentamente toda a lista de exercícios e familiarize-se com o código fornecido **antes** de começar a implementação.\n",
    "- Os locais onde você deverá escrever suas soluções estão demarcados com comentários `# YOUR CODE HERE` ou `YOUR ANSWER HERE`.\n",
    "- **Não altere o código fora das áreas indicadas, nem adicione ou remova células. O nome deste arquivo também não deve ser modificado.**\n",
    "- Antes de submeter, certifique-se de que o código esteja funcionando do início ao fim sem erros.\n",
    "- Submeta apenas este notebook com as suas soluções no Moodle.\n",
    "- Prazo de entrega: 15/04/2025. **Submissões fora do prazo terão uma penalização de -20% da nota final por dia de atraso.**\n",
    "- Utilize a [documentação do Gymnasium](https://gymnasium.farama.org/) para auxiliar sua implementação.\n",
    "- Em caso de dúvidas entre em contato pelo fórum \"Dúvidas com relação aos exercícios e trabalho de curso\" no moodle da Disciplina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea77-cdbe-474f-bc2f-95daa8d439c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31845cf7b26a7f7728c186a8c96638a3",
     "grade": false,
     "grade_id": "cell-f1f0ba316c0b79ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1557b9-966f-44c4-85be-cb75ffd2c95d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1098a937587b35eb2f0dae504db9be3",
     "grade": false,
     "grade_id": "cell-69a0af10519ed240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "O ambiente Frozen Lake é uma simulação clássica utilizada para treinamento de agentes em aprendizado por reforço. Neste ambiente, o agente navega por um lago congelado representado por um grid de tamanho $n \\times m$, com o objetivo de alcançar um alvo. O lago contém dois tipos de células: (1) células com gelo sólido, que são seguras para o agente se mover, e (2) as células com buracos, nas quais o agente cai e falha a missão. Embora o Gymnasium já possua uma implementação do Frozen Lake, neste exercício iremos implementá-lo do zero.\n",
    "\n",
    "No início de cada episódio, o agente é posicionado na célula $[0, 0]$ enquanto o alvo é posicionado na célula mais distante do agente, na posição $[n-1, m-1]$ em um mapa de tamanho $n \\times m$. A cada passo, o agente recebe uma observação indicando sua posição atual no lago e tem a possibilidade de escolher entre quatro ações possíveis: mover-se para cima, para baixo, para a esquerda ou para a direita. No entanto, devido à superfície escorregadia do lago, ele nem sempre se move na direção desejada, podendo acabar se movendo em uma direção perpendicular à escolhida. O agente recebe uma recompensa de 1 se alcançar o alvo e zero em todos os outros estados. Um episódio termina quando o agente alcança o objetivo ou cai na água.\n",
    "\n",
    "Neste exercício, vamos trabalhar sempre com o mesmo mapa $4 \\times 4$, representado na figura abaixo.\n",
    "\n",
    "![Frozen Lake Map](https://gymnasium.farama.org/_images/frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c46123-2985-4e8e-87b5-ed33a8fccfb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa4ec9fb33d5d4a8c2e5475b072f2e6f",
     "grade": false,
     "grade_id": "cell-6e7a4e34e7d477a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sua primeira tarefa será implementar o ambiente Frozen Lake utilizando o arcabouço fornecido pelo Gymnasium. Abaixo, você encontrará um código inicial que deverá ser utilizado em sua implementação. Siga essas instruções para garantir que seu código está de acordo com o esperado:\n",
    "\n",
    "1. Na função `__init__`, já definimos o mapa que será utilizado e armazenamos essa informação na variável `_description`. Nesse mapa, a letra 'S' representa a posição inicial do agente, a letra 'G' indica o alvo, as letras 'F' representam gelo sólido (que é seguro) e as letras 'H' marcam os buracos. No entanto, ainda é necessário adicionar mais algumas informações no ambiente, especificamente sobre a representação das observações e das ações. Embora existam várias maneiras de representar os espaços de observações e de ação, neste exercício, você deve usar a forma mais simples possível, que pode ser representada por um único valor discreto. Na função `__init__`, defina o espaço de observações e o espaço de ações, atribuindo-os às variáveis `self.observation_space` e `self.action_space`, respectivamente. Utilize apenas a classe `gymnasium.spaces.Discrete` nesta tarefa.\n",
    "\n",
    "2. Antes de prosseguirmos com as funcionalidades do gymnasium, vamos implementar algumas funções auxiliares para facilitar as próximas etapas. Implemente a função `_get_obs`, que retorna a observação atual do ambiente. Além disso, implemente a função `_set_state`, que recebe um valor inteiro correspondente a uma posição no lago e coloca o agente nesta localização.\n",
    "\n",
    "3. A função `reset` deve resetar o ambiente e inicializar um novo episódio, posicionando o agente na célula $[0, 0]$ e fazendo todos os ajustes internos necessários. Esta função deve retornar uma tupla contendo a observação inicial e as informações do ambiente. Neste exercício, vamos retornar um dicionario vazio `{}` para as informações. Lembre-se que a observação deve ser um único valor discreto, como definido no item 1. Implemente a função `reset`.\n",
    "\n",
    "4. A função `step()` é responsável por atualizar o ambiente com base na ação executada pelo agente. Ela recebe como entrada a ação escolhida pelo agente, um parâmetro seed e uma variável options, e calcula o novo estado atual com base na função de transição previamente definida. Neste exercício, você pode ignorar os parâmetros seed e options, pois não precisaremos deles. Neste ambiente que estamos desenvolvendo, o agente tem 80% de chance de se mover na direção desejada e 20% de chance de se mover em uma direção perpendicular à escolhida, distribuída igualmente entre os dois sentidos possíveis (10% para cada um). **As ações do agente devem ser representadas pelos valores 0 (mover-se para a esquerda), 1 (mover-se para baixo), 2 (mover-se para a direita) e 3 (mover-se para cima)**. Além disso, a função atribui uma recompensa ao agente e verifica se o episódio chegou ao fim. Implemente a função step() para que ela retorne a observação do estado atual, a recompensa recebida, um valor booleano indicando se o estado é terminal, um valor booleano informando se o episódio foi truncado e as informações do ambiente. Não se preocupe com estes dois últimos valores, nós não vamos utilizá-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c82e329-a086-4479-8375-bb1545073294",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ed0c0feb7c452a0631b25e2e0cd0e62",
     "grade": false,
     "grade_id": "cell-bd1f83bd5e0cfba6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e848a9-20d8-4688-a080-306628fa3b9c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f6883a4d3a67114700d7686062bf221",
     "grade": false,
     "grade_id": "cell-b8e10c8d02b6faa6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self._description = np.asarray([\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ], dtype='c')\n",
    "\n",
    "        self.n_rows, self.m_cols = self._description.shape\n",
    "\n",
    "        # Observation space id a n * m grid\n",
    "        self.observation_space = gym.spaces.Discrete(self.n_rows * self.m_cols)\n",
    "\n",
    "        # 0 = Left, 1 = Down, 2 = Right, 3 = Up\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        row, col = self.agent_pos\n",
    "        return row * self.m_cols + col\n",
    "\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        row = state // self.m_cols\n",
    "        col = state % self.m_cols\n",
    "        self.agent_pos = (row, col)\n",
    "\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.agent_pos = (0, 0)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        directions_and_moves = {\n",
    "        # Direction, (line, column). Not (x, y)\n",
    "            0: (0, -1),  # Left\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, 1),   # Right\n",
    "            3: (-1, 0)   # Up\n",
    "        }\n",
    "\n",
    "        perpendiculars_and_actions = {\n",
    "            0: [1, 3], # Down, Up\n",
    "            1: [0, 2], # Left, Right\n",
    "            2: [1, 3], # Down, Up\n",
    "            3: [0, 2]  # Left, Right\n",
    "        }\n",
    "\n",
    "        probability_of_actions = [0.8, 0.1, 0.1]\n",
    "        possible_actions = [action] + perpendiculars_and_actions[action]\n",
    "\n",
    "        # The real action will be defined based on the defined probabilities\n",
    "        action = np.random.choice(possible_actions, p=probability_of_actions)\n",
    "\n",
    "        new_row = self.agent_pos[0] + directions_and_moves[action][0]\n",
    "        new_col = self.agent_pos[1] + directions_and_moves[action][1]\n",
    "\n",
    "        # Check if the new position is valid\n",
    "        if new_row > 0 and new_row < self.n_rows:\n",
    "            if new_col > 0 and new_col < self.m_cols:\n",
    "                self.agent_pos = (new_row, new_col)\n",
    "\n",
    "        new_cell = self._description[self.agent_pos[0], self.agent_pos[1]].decode()\n",
    "\n",
    "        if new_cell == 'G':\n",
    "            reward = 1\n",
    "            is_terminal = True\n",
    "\n",
    "        elif new_cell == 'H':\n",
    "            reward = 0\n",
    "            is_terminal = True\n",
    "\n",
    "        else:\n",
    "            reward = 0\n",
    "            is_terminal = False\n",
    "\n",
    "        is_truncated = None\n",
    "        environment_info = {}\n",
    "\n",
    "        return self._get_obs(), reward, is_terminal, is_truncated, environment_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbfd643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'S'\n"
     ]
    }
   ],
   "source": [
    "# REMOVE\n",
    "\n",
    "frozenLake = FrozenLake()\n",
    "print(frozenLake._description[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8e99e-94bb-426f-a7e0-106dad2769b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e78bddd1db45c2e8b9e54c20f0791463",
     "grade": true,
     "grade_id": "cell-20901ef53a25a2b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c074aab-2482-41eb-92ba-e3eb25527a57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a6bd70f04426794643b62bbf2b83645",
     "grade": true,
     "grade_id": "cell-461b638c8fbd4a98",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bacf40-6efa-4aee-92ff-5dc68195134a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cd1b48396fa68dd2fbde617576e5447",
     "grade": true,
     "grade_id": "cell-37b8d8892adec617",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673e83f-1c2d-4934-bacb-0e879fcf6eb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d23f4e00da51ae33b33e3136ac5b0f",
     "grade": true,
     "grade_id": "cell-b30cbb7c1fa808b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d278f-5787-44d4-95b1-7e537b15db8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977788f0df2f7412652fd64ee795ff21",
     "grade": false,
     "grade_id": "cell-8b132c80e15a2de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2281a3b1-ce03-42e0-9a4a-c7760aa904be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfacc86e8ff18125c69aa0171d1d7adf",
     "grade": false,
     "grade_id": "cell-e6a8d0aebce03142",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Agora que estamos familiarizados com o ambiente FrozenLake, nosso objetivo é encontrar uma política ótima para esse ele.  Desta vez, utilizaremos a versão oficial do Frozen Lake disponibilizado pelo Gymnasium. Ele possui algumas propriedades que facilitarão as próximas implementações. **Fique atento às transições do ambiente, pois elas são diferentes das do exercício anterior.** Neste ambiente oficial, o agente possui apenas $1/3$ de chance de se mover na direção desejada e $2/3$ de chance de se mover perpendicularmente. Sua tarefa será implementar o algoritmo *Policy Iteration*, conforme ilustrado abaixo.\n",
    "\n",
    "![Policy Iteration](policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cbfcc-d78a-4409-91b7-ae40d26d254c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248bbc9df645c2d3c87ae09f1c751a52",
     "grade": false,
     "grade_id": "cell-80af17ade7f65c5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "5. A implementação será realizada em etapas. Comece implentando a função `init_policy_iteration`, que inicializa e retorna dois arrays. O primeiro array armazenará os valores esperados de cada estado $V(s)$, enquanto o segundo conterá a política do agente: para cada estado, ele indicará a ação que o agente deve realizar. Ambos os arrays devem ser inicializados com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afa70b-1d9e-4cfd-b026-9669cb329d3f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1465ddc541cd84973baead52e1296a77",
     "grade": false,
     "grade_id": "cell-0fb77ab1fc981da0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_policy_iteration(env: gym.Env) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bba5a-50c6-44f4-aafb-b82289b4cbc4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf58ea50c084b3cd43cdd7e31240e66f",
     "grade": true,
     "grade_id": "cell-2cc190091c84d0c8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61c76e-1a7b-4fa0-afe7-8aa4296a852a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcda365a4d97947d565e8c44d241610c",
     "grade": false,
     "grade_id": "cell-833ab69b40216f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "6. Agora, vamos computar o valor esperado $V(s) = \\sum_{s', r}p(s',r|s, a)[r + \\gamma V(s')]$. Implemente a função `compute_expected_value`que recebe como parâmetros o ambiente, o vetor $V$, um estado, uma ação, o valor de $\\gamma$ (fator de desconto), e retorna o valor esperado. Não altere os valores de $V$ nesta função.\n",
    "\n",
    "**Importante:** A variável `env.unwrapped.P[state][action]` contém as transições do ambiente, retornando uma lista com todas as transições possíveis para o par (state, action). Cada elemento dessa lista inclui, na seguinte ordem: a probabilidade da transição, o estado $s'$ alcançado, a recompensa recebida e um indicador de estado terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88dd79-de71-4c91-bc21-13f6db8f350e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086e19afee3929471d473c6b70e42a7e",
     "grade": false,
     "grade_id": "cell-fbb3d72642775bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_expected_value(env: gym.Env, V: np.ndarray[float], state: int, action: int, gamma: float) -> float:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ff99b-5596-49f5-a25a-9d702a40330a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5ee209caca42d4bbfefaa0e6b626f72",
     "grade": true,
     "grade_id": "cell-4602ffd22effb339",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c4f38-ec8c-4d43-ac71-68243b719428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd97b9bb7399cf9d8ad4b4f9d324c34",
     "grade": false,
     "grade_id": "cell-b94451eaf15d9d58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "7. O pŕoximo passo será avaliar a política do agente. Implemente o loop de avaliação de política do policy iteration na função `evaluate_policy`. Ela receberá o ambiente, a política do agente, o vetor $V$, o valor $\\gamma$, e o valor $\\theta$. Esta função não precisa retornar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b124c-9ed3-47aa-a7bb-e1a198bdb87b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc74a4d30b19f68b2a836179a817e2f6",
     "grade": false,
     "grade_id": "cell-8df0495bf82eaaf7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float, theta: float) -> None:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fdf4d-13ed-449a-b2f7-6bc3605e11e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b53d720e3855417974c16102978b5913",
     "grade": true,
     "grade_id": "cell-9e7ec2bd9abe2669",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8faf3-7d83-4c24-8f38-110981cf296b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a2d75b60f5350454137839a5e61ce25",
     "grade": false,
     "grade_id": "cell-deee4c0a66b4ef76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "8. A seguir, vamos implementar a atualização da política. Na função `improve_policy` implemente uma iteração da atualização da política. Ela recebe o ambiente, a política do agente, o vetor $V$, e o valor $\\gamma$. Ela deverá retornar um booleano indicando se política está estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818354d1-3e10-4ac8-96cd-51eff85c54a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0338436f555ec731b7fee10a05a929d5",
     "grade": false,
     "grade_id": "cell-4604a015625bbd5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(env: gym.Env, policy: np.ndarray[int], V: np.ndarray[float], gamma: float) -> bool:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b08de-11e1-48b1-87a7-5da2b6919654",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c2699258747a8002aed18cd371c6345",
     "grade": true,
     "grade_id": "cell-3d9c290251abe874",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886671-c6a1-4bf8-8258-137c8030d8fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e9728ce314ad841c660af9d5745807",
     "grade": false,
     "grade_id": "cell-59ebc175679651bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A célula abaixo implementa a estrutura do algoritmo *Policy Iteration* utilizando as funções desenvolvidas nas etapas anteriores. Não é necessário realizar nenhuma implementação nesta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab5bc4-ac2a-4a79-b5d5-7b6d668805b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc9300dfe827083e071ca31f8ddcefd",
     "grade": false,
     "grade_id": "cell-250e7d4c7bc0cf97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env: gym.Env, gamma: float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    V, policy = init_policy_iteration(env)\n",
    "    \n",
    "    while True:\n",
    "        evaluate_policy(env, policy, V, gamma, theta)\n",
    "        policy_stable = improve_policy(env, policy, V, gamma)\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef24a7-7071-4c51-926c-32b9e9c39c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fda197275df7ccba7b02322c88365343",
     "grade": false,
     "grade_id": "cell-ed343128eb786bed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def print_policy(env:gym.Env, policy: np.ndarray[int]):\n",
    "    action_map = ['←', '↓', '→', '↑']\n",
    "    policy_grid = np.full((4, 4), \"\")\n",
    "    \n",
    "    for index, action in enumerate(policy):\n",
    "        row, col = divmod(index, 4)\n",
    "        if env.unwrapped.desc[row, col] == b'H':\n",
    "            policy_grid[row, col] = '▢'\n",
    "        elif env.unwrapped.desc[row, col] == b'G':\n",
    "            policy_grid[row, col] = '◎'\n",
    "        else:\n",
    "            policy_grid[row, col] = action_map[action]\n",
    "    np.savetxt(sys.stdout, policy_grid, fmt='%s', delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5ed1-6b65-425f-894e-769d90603bdf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6843475678b0a4efacd009ff86f9cf5",
     "grade": false,
     "grade_id": "cell-98df69850425fb45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Observe a política obtida pelo policy iteration na célula abaixo e responda as perguntas a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d8872-c213-44e4-ae3b-d187c7a7f097",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40024ab1474d70ac84c2766b4f23cbc9",
     "grade": false,
     "grade_id": "cell-d1021093fae62b6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "theta = 1e-8\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\")\n",
    "V, policy = policy_iteration(env, gamma, theta)\n",
    "print_policy(env, policy)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbcaa6-60ba-4c8c-942e-6c01ceec48d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6381f9e262608986ecce118c87ab010b",
     "grade": false,
     "grade_id": "cell-acf652424f23bbef",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "9. A política aprendida está de acordo com o comportamento esperado para este problema? Justifique sua resposta com base nos resultados observados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79354419-a365-4ea7-b089-a6d316675555",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "951c7580da3b3f980c9b90c043bc62ad",
     "grade": true,
     "grade_id": "cell-60cb0c5f9361510e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30683f-12d7-409b-8b21-1204d901a6d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bf86dbc59d98ea3845d5914cb99fbcd",
     "grade": false,
     "grade_id": "cell-18520062e98bbf2a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "10. Quais estratégias poderiam ser adotadas para melhorar a política obtida?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd0586-bf1c-46ac-89ea-cded1aed1076",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e509d072ff718ea76b9b4a74d928d190",
     "grade": true,
     "grade_id": "cell-87774430d929c3ac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3731f7-5e61-4b1c-adc9-ce021b8345ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dbe936a18f99cac42ecde3ff233907a",
     "grade": true,
     "grade_id": "cell-a422263f5dd5254d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297afae4-639a-4741-811a-1693ce726413",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75f8abf27b293dd60179ff3844f5c8f0",
     "grade": false,
     "grade_id": "cell-5e9663f3341ea0cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf17ac-9e34-4b94-a8c0-abefbd22c201",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "213cec389fa200d841b8e593f5a6d5f5",
     "grade": false,
     "grade_id": "cell-da562c89000eeffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Neste exercício vamos encontrar uma política ótima para o Frozen Lake utilizando o algoritmo *Value Iteration* como descrito abaixo.\n",
    "\n",
    "![Value Iteration](value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d4db7-80f5-4964-aa87-bf561413c5e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b95f3735c85d401b5746cc8c31e8ecf0",
     "grade": false,
     "grade_id": "cell-179ab2fb26b003ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "11. Novamente vamos dividir este exercícios em etapas menores. O primeiro passo consiste em inicializar o vetor $V$, que armazenará os valores esperados para cada estado. Para isso, implemente a função `init_value_iteration`, que recebe um ambiente como parâmetro e retorna o vetor $V$. Este vetor deve ser inicializado com valores zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e2c68-c87e-4342-8ca6-57a43685180b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257304c2b6c04f1ce1743dde7e5095de",
     "grade": false,
     "grade_id": "cell-80a50671b2e72667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def init_value_iteration(env: gym.Env) -> np.ndarray[float]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7a93d-d8e0-4bbc-b720-405af9a75d26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8212847f0a2b42dee8c85c2481ec26a9",
     "grade": true,
     "grade_id": "cell-4801e75927f5a234",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62953af-0ac2-4064-9731-be0d11e6285a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddb1ff063720a47ee2c5cabb6635d921",
     "grade": false,
     "grade_id": "cell-4e33e273961d7334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "12. Agora, vamos gerar uma política determinística a partir de um vetor $V$, conforme definido pela equação $\\pi(s)= \\textrm{argmax}_a \\sum_{s', r}p(s', r|s, a)[r + \\gamma V(s')]$. Implemente a função `generate_policy`, que recebe um ambiente e um vetor $V$, retornando a política determinística resultante.\n",
    "\n",
    "**Dica:** Utilize a função `compute_expected_value` do exercício anterior para facilitar sua implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3a710-e02c-46be-a93b-56683cfe6593",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee4fc2ed2d51ff97989aaddc8706c51",
     "grade": false,
     "grade_id": "cell-41eb7e70f58ee606",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_policy(env: gym.Env, V: np.ndarray[float], gamma: float) -> np.ndarray[int]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f149bf-e88c-458a-a524-60fa22549220",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41b02f8e08010941a21585dc34c4b474",
     "grade": true,
     "grade_id": "cell-f5ef610430057c94",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb76228-99f7-4eb3-a265-e05e0ade9a3a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afa3c95719710b985e9708d5c79e5566",
     "grade": false,
     "grade_id": "cell-eef5f2b3ef2b657d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "13. Por fim, implemente o loop principal do *Value Iteration* na função `value_iteration`. Ela deverá retornar, nesta ordem, o array de valores $V$ e a política obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4be75-e22e-40f7-aca3-31f7b810c32d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1207e1ee95de47ccab3943f810509cc9",
     "grade": false,
     "grade_id": "cell-c48b9185009da819",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env: gym.Env, gamma:float, theta: float) -> tuple[np.ndarray[float], np.ndarray[int]]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f621f3f-9261-4f75-93fa-a511c3013aad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80ae5aba497bb9f276720a59bf0f8569",
     "grade": false,
     "grade_id": "cell-3a5e438c6988c441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Certifique-se que seu algoritmo esteja funcionando corretamente na célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13ad40-daa8-4c50-92c2-92336378d91f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c13fe663a94a1e6060ae553e4c5678",
     "grade": false,
     "grade_id": "cell-65ff2786217d46c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "theta = 1e-8\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\")\n",
    "V, policy = value_iteration(env, gamma, theta)\n",
    "print_policy(env, policy)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7814681-8791-4066-9205-6bd78043f7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97485126d30340491d32b30595dda556",
     "grade": true,
     "grade_id": "cell-90866ba893778afb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Não altere ou remova esta célula"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
